{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Mechansim of induction heads and common subspace propoerty\n",
    "### This notebook only looks at GPT-2 \n",
    "\n",
    "**The overall plan is to provide evidence for the four main arguments**.\n",
    "\n",
    "1. Multiplying weight matrix W_QK and W_OV yields large diagonal values\n",
    "   \n",
    "2. The top right singular subspace of W_QK matches the top left singular subspaec of W_OV\n",
    "   \n",
    "3. (Invariance) Shuffling induction heads do not significantly impact performance of copying\n",
    "\n",
    "4. (Common subspace as composition pathway) Project out common subspace in induction heads disables copying, while only keeping common subspace is sufficient for copying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yiqiaoz/opt/anaconda3/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import seaborn as sns\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from transformers import logging\n",
    "from transformers import set_seed\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from analysis_utils import create_folder, qkov_matching, subspace_matching\n",
    "\n",
    "set_seed(2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: run Step 1-1-1, 1-1-2, 1-1-3, 1-1-4, 1-1-5 to get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calulating embeddings and matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-1-1: loading GPT-2 and setting global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 12, 15, 15)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = GPT2Config()\n",
    "model = GPT2Model.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "configuration = model.config\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "vocab_size = 50257\n",
    "T0 = 5\n",
    "\n",
    "sample_int = np.random.randint(low=0,high=vocab_size,size=T0).repeat(3).reshape(T0,-1).T.ravel()\n",
    "input_ids = torch.Tensor(sample_int).long().unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids) # (num_layer, num_head, seq_length, seq_length) = (12, 12, 15, 15)\n",
    "len(output.attentions), output.attentions[0].size()\n",
    "\n",
    "attentions = np.array([a.detach().numpy()[0] for a in output.attentions])\n",
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-1-2: Calculating embeddings for each sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = 12\n",
    "num_heads = 12\n",
    "d_model = 768\n",
    "d_head = d_model // num_heads\n",
    "seq_len = input_ids.size(1)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "hiddens_all = torch.zeros(num_layer+1, 6, seq_len, d_model)\n",
    "\n",
    "h = model.wte(input_ids)\n",
    "pos = torch.arange(0, seq_len, dtype=torch.long).unsqueeze(0)\n",
    "h = model.drop(model.wte(input_ids) + model.wpe(pos))\n",
    "hiddens_all[0] = h.squeeze()\n",
    "for layer in range(num_layer):\n",
    "    h2 = model.h[layer].ln_1(h) # LayerNorm_1 output embeddings, (seq_length, d_model)\n",
    "    h4 = model.h[layer].attn(h2)[0]  # Self-attention output embeddings (seq_length, d_model)\n",
    "    h5 = h + h4 # Adding with identity component, (seq_length, d_model)\n",
    "    h6 = model.h[layer].ln_2(h5) # LayerNorm_2 output embeddings, (seq_length, d_model)\n",
    "    h = model.h[layer](h)[0] # Next-layer embeddings, (seq_length, d_model)\n",
    "\n",
    "    hiddens_all[layer+1, 0] = h.squeeze() \n",
    "    hiddens_all[layer, 1] = h2.squeeze() \n",
    "    # hiddens_all[layer, 2] = torch.tensor(attentions[layer,layer,:,:]) @ h2.squeeze()  # NOT CORRECT\n",
    "    hiddens_all[layer, 3] = h4.squeeze() \n",
    "    hiddens_all[layer, 4] = h5.squeeze() \n",
    "    hiddens_all[layer, 5] = h6.squeeze()    \n",
    "    \n",
    "def cosine_sim(x, y, prec_digit=4):\n",
    "    x, y = torch.tensor(x), torch.tensor(y)\n",
    "    out = torch.sum(x * y) / (torch.norm(x) * torch.norm(y))\n",
    "    return np.around(out.numpy(force=True), decimals=prec_digit)\n",
    "\n",
    "def matrix_mask(d, return_one=True, offsets=[0]):\n",
    "    offsets = [torch.tensor(offset) for offset in offsets]\n",
    "    mask = torch.zeros(d, d, dtype=torch.bool)\n",
    "    for offset in offsets:\n",
    "        mask += torch.diag(torch.ones(d-torch.abs(offset), dtype=torch.bool), offset)\n",
    "    mask = mask if return_one else ~mask\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-1-3: Calculating QK and OV matrices for all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = model.wte.weight\n",
    "vals_flip, vecs_flip = np.linalg.eigh((W0.T @ W0).numpy(force=True))\n",
    "vals, vecs = vals_flip[::-1], vecs_flip[:,::-1]\n",
    "\n",
    "W_all = torch.zeros(num_layer, num_heads, 4, d_model, d_head)\n",
    "\n",
    "for layer in range(num_layer):\n",
    "    W_q, W_k, W_v = model.h[layer].attn.c_attn.weight.split(d_model, dim=1)\n",
    "    W_q = W_q.view(d_model, num_heads, d_model//num_heads)\n",
    "    W_k = W_k.view(d_model, num_heads, d_model//num_heads)\n",
    "    W_v = W_v.view(d_model, num_heads, d_model//num_heads)\n",
    "    W_o = model.h[layer].attn.c_proj.weight.view(num_heads, d_model//num_heads, d_model)\n",
    "    \n",
    "    for head in range(num_heads): \n",
    "        W_all[layer, head, 0] = W_q[:,head,:] # (d_model, d_head)\n",
    "        W_all[layer, head, 1] = W_k[:,head,:] # (d_model, d_head)\n",
    "        W_all[layer, head, 2] = W_v[:,head,:] # (d_model, d_head)\n",
    "        W_all[layer, head, 3] = W_o[head,:,:].T # (d_model, d_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-1-4: Measuring induction head (ranking most likely induction head to most unlikely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 5 Head 1: score 0.9953718118369579\n",
      "Layer 5 Head 5: score 0.990281829237938\n",
      "Layer 6 Head 9: score 0.9886472076177597\n",
      "Layer 7 Head 10: score 0.9395123064517975\n",
      "Layer 5 Head 8: score 0.9212846681475639\n",
      "Layer 10 Head 7: score 0.9204489514231682\n",
      "Layer 5 Head 0: score 0.8876477140933275\n",
      "Layer 7 Head 2: score 0.8102711588144302\n",
      "Layer 10 Head 1: score 0.8059151142835617\n",
      "Layer 9 Head 9: score 0.7934778571128845\n",
      "Layer 9 Head 6: score 0.7573517456650734\n",
      "Layer 11 Head 10: score 0.7551812022924423\n",
      "Layer 6 Head 1: score 0.6811700388789177\n",
      "Layer 10 Head 6: score 0.6629793018102645\n",
      "Layer 8 Head 1: score 0.6548116706311703\n",
      "Layer 10 Head 0: score 0.6461946688592434\n",
      "Layer 7 Head 11: score 0.6174160994589328\n",
      "Layer 8 Head 6: score 0.598575821518898\n",
      "Layer 10 Head 10: score 0.5727810949087143\n",
      "Layer 8 Head 11: score 0.5598440669476986\n",
      "Layer 9 Head 1: score 0.52265305519104\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'simple_visz'\n",
    "create_folder(dir_name)\n",
    "filename = os.path.join(dir_name, f'induction_head_scores.txt')\n",
    "\n",
    "scores = np.zeros((num_layer, num_heads))\n",
    "for layer in range(num_layer):\n",
    "    for head in range(num_heads):\n",
    "        A = attentions[layer, head]\n",
    "        A_adjusted = np.zeros((seq_len, seq_len))\n",
    "        A_adjusted[1:, 1:] = A[1:, 1:] / np.sum(A[1:, 1:], axis=1, keepdims=True)\n",
    "        diag1 = np.diag(A_adjusted, -(T0-1))[1:]\n",
    "        diag2 = np.diag(A_adjusted, -(2*T0-1))[1:]\n",
    "        diag = np.concatenate((diag1[:-T0], diag1[-T0:] + diag2))\n",
    "        scores[layer, head] = np.mean(diag)\n",
    "        \n",
    "idx_sort = np.argsort(scores, axis=None)[::-1]\n",
    "IH_list = [[idx_sort[j] // num_heads, idx_sort[j] % num_heads] for j in range(len(idx_sort))]\n",
    "\n",
    "with open(filename, 'w') as file:\n",
    "    print(f'Ranking induction heads (most likely to unlikely) by attention scores', file=file)\n",
    "    for j, pair in enumerate(IH_list):\n",
    "        layer, head = pair[0], pair[1]\n",
    "        print(f'Layer {layer} Head {head}: score {scores[layer, head]}', file=file)\n",
    "        if j <= 20:\n",
    "            print(f'Layer {layer} Head {head}: score {scores[layer, head]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1-1-5: Measuring Shifting Head (ranking most likely induction head to most unlikely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 4 Head 11: score 0.9993197276042058\n",
      "Layer 5 Head 6: score 0.7670074793008658\n",
      "Layer 6 Head 8: score 0.744445766393955\n",
      "Layer 3 Head 7: score 0.6400530796784621\n",
      "Layer 3 Head 3: score 0.5796717221920307\n",
      "Layer 2 Head 2: score 0.5276686285550778\n",
      "Layer 7 Head 0: score 0.5010625811723562\n",
      "Layer 2 Head 5: score 0.4850600384748899\n",
      "Layer 3 Head 2: score 0.4379009088644615\n",
      "Layer 3 Head 8: score 0.42955268919467926\n",
      "Layer 4 Head 3: score 0.41924098707162416\n",
      "Layer 5 Head 4: score 0.4164687842130661\n",
      "Layer 8 Head 7: score 0.41152643813536716\n",
      "Layer 2 Head 9: score 0.40762703808454365\n",
      "Layer 3 Head 6: score 0.3934387805370184\n",
      "Layer 2 Head 8: score 0.3858960825376786\n",
      "Layer 4 Head 6: score 0.37324318519005406\n",
      "Layer 7 Head 8: score 0.3646498970114268\n",
      "Layer 2 Head 3: score 0.35175178200006485\n",
      "Layer 6 Head 0: score 0.33364908970319307\n",
      "Layer 5 Head 2: score 0.32602146153266615\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'simple_visz'\n",
    "create_folder(dir_name)\n",
    "filename = os.path.join(dir_name, f'shifting_head_scores.txt')\n",
    "\n",
    "scores = np.zeros((num_layer, num_heads))\n",
    "for layer in range(num_layer):\n",
    "    for head in range(num_heads):\n",
    "        A = attentions[layer, head]\n",
    "        A_adjusted = np.zeros((seq_len, seq_len))\n",
    "        A_adjusted[1:, 1:] = A[1:, 1:] / np.sum(A[1:, 1:], axis=1, keepdims=True)\n",
    "        diag = np.diag(A_adjusted, -1)[1:]\n",
    "        scores[layer, head] = np.mean(diag)\n",
    "        \n",
    "idx_sort = np.argsort(scores, axis=None)[::-1]\n",
    "Shifting_list = [[idx_sort[j] // num_heads, idx_sort[j] % num_heads] for j in range(len(idx_sort))]\n",
    "\n",
    "with open(filename, 'w') as file:\n",
    "    print(f'Ranking shifting heads (most likely to unlikely) by attention scores', file=file)\n",
    "    for j, pair in enumerate(Shifting_list):\n",
    "        layer, head = pair[0], pair[1]\n",
    "        print(f'Layer {layer} Head {head}: score {scores[layer, head]}', file=file)\n",
    "        if j <= 20:\n",
    "            print(f'Layer {layer} Head {head}: score {scores[layer, head]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below we provide evidence for our claims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim 1: large diagonal values in W_qkov\n",
    "\n",
    "**We plan to select a few representative heatmaps of W_qkov to show.**\n",
    "\n",
    "**Further, we plan to calculate and perhaps show some statistics**\n",
    "\n",
    "1. qkov_matching_summary calculates z-scores of every top (IH, Shifting head) pair, and plots a heatmap. A value bigger than 2 suggests that diagonal line is significantly larger than other off-diagonal values.\n",
    "\n",
    "2. We need to pay attention to how to choose the thresholds for determining \"top\" IH and Shifting heads. Here for simplicity, I just choose top-10 heads. Perhaps there are more clear induction heads and fewer clear shifting heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_folder('Figs')\n",
    "create_folder('Figs/diagonal')\n",
    "\n",
    "res = qkov_matching(W_all, IH_list[:10], 'Figs/diagonal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analysis_utils import qkov_matching_summary\n",
    "K0, K1 = 20, 20\n",
    "scores = qkov_matching_summary(W_all, IH_list[:K0], Shifting_list[:K1], 'Figs/diagonal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim 2: subspace matching\n",
    "\n",
    "**Generalized cosine similarity**: to measure how similar two subspaces are, I use two methods named \"largest\" and \"mean\". Both methods give values between 0 (orthogonal) to 1 (aligned).\n",
    "\n",
    "- \"largest\" is a favorable score that finds the best vectors in each subspaces to maximize the regular cosine similarity\n",
    "- \"mean\" reflects how similar a random vector in a subspace is to a random vector in another subspace\n",
    "\n",
    "**Three matching measurements**: Among induction heads, among shifting heads, and between IH and shifting\n",
    "\n",
    "**Finding**\n",
    "- Under \"largest\", many subspaces match well (score > 0.8)\n",
    "- Under \"mean\", many subspaces are positively correlated (0.2--0.6). It suggests subspaces are not perfectly aligned, but there are nontrivial correlation and much better than random subspaces (which are almost orthogonal, values << 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.80it/s]\n",
      "100%|██████████| 50/50 [00:28<00:00,  1.77it/s]\n",
      "100%|██████████| 50/50 [00:25<00:00,  1.93it/s]\n"
     ]
    }
   ],
   "source": [
    "K = 10\n",
    "s_match1, match_baseline1 = subspace_matching(W_all, 'Figs/subspace_matching_IH', IH_list=IH_list[:K], ranks=[2, 3, 5, 10])\n",
    "s_match2, match_baseline2 = subspace_matching(W_all, 'Figs/subspace_matching_SH', Shifting_list=Shifting_list[:K], ranks=[2, 3, 5, 10])\n",
    "s_match3, match_baseline3 = subspace_matching(W_all, 'Figs/subspace_matching_IH_SH', IH_list=IH_list[:K], Shifting_list=Shifting_list[:K], ranks=[2, 3, 5, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim 3: invariance under shuffling\n",
    "\n",
    "**Error reporting for edited models**. I used a batch of repeated random tokens to measure how well an edited model performs copying.\n",
    "\n",
    "\n",
    "**Ideal plots**. We hope to see prediction errors remain low under random shuffling, and random baseline has high errors. As we increase more heads to shuffle, prediction errors increase.\n",
    "\n",
    "**Issues**.\n",
    "\n",
    "1. **High variability**. Because each edited model only use one random permutation, its performance depends on how several important heads are permuted. To draw conclusions, we should run many independent experiments. Below for conveience I use run one experiment for each K.\n",
    "\n",
    "2. **Results about shifting heads are weird**. Is it due to variability? Is there a bug? When I increase K, errors of original, edited, random baseline are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [20:15<00:00, 75.97s/it]\n"
     ]
    }
   ],
   "source": [
    "from analysis_utils import exchange_edit, pred_probs, shuffle_exp\n",
    "\n",
    "seg_len = 20 # repeating segment length\n",
    "for K in tqdm(range(5, 21)):\n",
    "    K0, K1 = K, K\n",
    "    save_dir = 'Figs/shuffle'\n",
    "    probs_QK, errs_QK = shuffle_exp(model, configuration, seg_len, vocab_size, IH_list[:K0], save_dir, component='QK')\n",
    "    probs_OV, errs_OV = shuffle_exp(model, configuration, seg_len, vocab_size, Shifting_list[:K1], save_dir, component='OV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Claim 4: projecting weight matrix\n",
    "\n",
    "- Experiments on induction heads suggest common subspace of rank ~ 50 is crucial for copying.\n",
    "\n",
    "- Experiments on shifting heads are bad. I don't know if there is a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [07:14<00:00, 21.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from analysis_utils import project_exp\n",
    "\n",
    "save_dir = 'Figs/project'\n",
    "seg_len = 20\n",
    "probs, errs = project_exp(model, configuration, W_all, seg_len, vocab_size, IH_list[:10], IH_list[:30], \n",
    "                          save_dir, component='QK', project_out=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [07:05<00:00, 21.28s/it]\n"
     ]
    }
   ],
   "source": [
    "probs, errs = project_exp(model, configuration, W_all, seg_len, vocab_size, IH_list[:10], IH_list[:30], \n",
    "                          save_dir, component='QK', project_out=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [06:58<00:00, 20.94s/it]\n",
      "100%|██████████| 20/20 [06:52<00:00, 20.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from analysis_utils import project_exp\n",
    "\n",
    "save_dir = 'Figs/project'\n",
    "seg_len = 20\n",
    "\n",
    "probs, errs = project_exp(model, configuration, W_all, seg_len, vocab_size, IH_list[:10], Shifting_list[:30], \n",
    "                          save_dir, component='OV', project_out=True, max_rank=100, step=5)\n",
    "probs, errs = project_exp(model, configuration, W_all, seg_len, vocab_size, IH_list[:10], Shifting_list[:30], \n",
    "                          save_dir, component='OV', project_out=False, max_rank=100, step=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
